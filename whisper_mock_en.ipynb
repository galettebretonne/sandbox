{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galettebretonne/sandbox/blob/master/whisper_mock_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# „ÄêMaster„Äë whisper-mock\n",
        "Whisper is a general-purpose speech recognition model open-sourced by OpenAI.\n",
        "\n",
        "## üìñ How to use\n",
        "1. Run \"Setting up\".\n",
        "2. Open the folder icon from the left sidebar.\n",
        "3. Upload audio files into the `content`.\n",
        "4. Input the audio file name into `fileName`.\n",
        "5. Select output language.\n",
        "5. Run \"Transcription\"."
      ],
      "metadata": {
        "id": "zw5ButypVydc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWIl4Ys54Ce6"
      },
      "outputs": [],
      "source": [
        "#@title Setting up\n",
        "# Install packages\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "import os\n",
        "\n",
        "# Add folders\n",
        "checkContentFolder = os.path.exists(\"content\")\n",
        "checkDownLoadFolder = os.path.exists(\"download\")\n",
        "if not checkContentFolder:\n",
        "  os.mkdir(\"content\")\n",
        "if not checkDownLoadFolder:\n",
        "  os.mkdir(\"download\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Transcription\n",
        "import whisper\n",
        "\n",
        "fileName = \"01 Rolling in the Deep.flac\"#@param {type:\"string\"}\n",
        "lang = \"en\"#@param [\"en\", \"fr\"]\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Load audio\n",
        "audio = whisper.load_audio(f\"content/{fileName}\")\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "# Output the recognized text\n",
        "options = whisper.DecodingOptions(language=lang, without_timestamps=True)\n",
        "result = whisper.decode(model, mel, options)\n",
        "print(result.text)\n",
        "\n",
        "# Write into a text file\n",
        "with open(f\"download/{fileName}.txt\", \"w\") as f:\n",
        "  f.write(f\"‚ñº Transcription of {fileName}\\n\")\n",
        "  f.write(result.text)"
      ],
      "metadata": {
        "id": "scAiM8ug_s1M",
        "outputId": "d1d5089e-56aa-4d2a-f963-bd4dd43fc52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There's a fire starting in my heart reaching a fever pitch and it's bringing me out the dark Finally I can see you crystal clear Go ahead and sell me out and I'll lay your ship back See how I live with every piece of you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\"/content/1-15 Le Blouson noir.mp3\",language=\"Fr\")\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "sRVQ_bgALmpl",
        "outputId": "f70eeca5-8142-41d8-b1e2-c216fef89980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ah ouais... Eh on fait pas de mal merde ! On est une bande de jeunes on se fond la gueule. Eh je vois √ßa venir les sorties avec Bob ! C'est Robert on l'appelle Bob. On aurait vu √ßa √† la crise ! On a foutu la merde on s'est fondu la gueule ! On est arriv√© dans une bo√Æte le barman on aurait dit une gauzesse. Oh la vache ! Il voulait pas nous servir Bob il est arriv√© il a pris le tabouret il a balanc√© dans les bouteilles. La crise ! T'es con vous auriez d√ª venir ! Ils ont appel√© les flics on avait juste le temps de se casser. Et apr√®s dans la rue on a rencontr√© une gauzesse mixteau compl√®tement bourr√©e. Avec des yeux de chatte maquill√©e vulgaire ! On l'a emmen√© dans les poubelles on a tir√© √† quatre. T'es con vous auriez d√ª venir ! Et Bobby √©tait tellement bourr√© il pouvait pas. Ah la crise ! Et puis apr√®s elle s'est mise √† gueuler parce qu'on fouillait dans la bo√Æte. Et puis elle a mis son sac. On s'est cass√©. Non mais on d√©conne comme √ßa mais on est vachement gentil tu vois parce que je vois. Remarque y a des mecs qui nous cherchent aussi. Parce que Bob bon si tu veux il est sympa si tu veux. Mais enfin c'est un mec quand il t'attaque pas toi il cherche √† se d√©fendre toi. Tu vois par exemple l'autre jour il √©tait avec Ginette devant le tabac ils discutaient. Bon dans un sens ils bouchaient l'entr√©e. Si on veut. Mais de l√† √† ce qu'un grossier soit mal poli. Y a un mec qui s'am√®ne genre blazer 16√®me propre sur lui petit col gentil. Il s'am√®ne il dit √† Ginette pardon mademoiselle excusez moi est-ce que vous pourriez vous d√©ranger l√©g√®rement s'il vous pla√Æt. Afin que je puisse en rasant le mur entrer dans le tabac sans vous d√©ranger. Bobby il fait ouais. Il dit tout de suite que ma femme est grosse. T'as vu ce qu'il a mis dans la gueule. Il a refait toute l'aventure. Moi j'√©tais l√† j'ai juste mis un coup de pompoudeux pour pas laisser un copain tout seul dans la merde. Non Bobby en plus faut le conna√Ætre. Moi c'est un mec j'√©tais de Choretta. Je suis all√© chez lui j'√©tais si √©tat. C'est un mec qui a fait une tour Eiffel en allumettes d'un m√®tre cinquante grande comme √ßa. Quand tu vois √ßa tu te demandes parce que. Tu vois √ßa tu te demandes parce que. Tu vois √ßa tu te demandes parce que. Quand tu vois √ßa tu te demandes parce que. Si tu veux parce que l√† le. Parce que des fois. Tu te demandes. Tu te dis mais est-ce possible que √ßa soit le m√™me mec qui attaque les vieilles dans la rue avec une matraque. Moi j'√©tais si √©tat j'√©tais de Choretta. Et puis c'est un mec c'est pour un con il lit les bouquins et tout. Alors j'ai regard√© dans sa biblioth√®que. Il lit les bouquins je comprends m√™me pas le titre. Alors t'es qu'√† voir. Et il a r√©fl√©chi sur sa condition et tout. Il dit toujours. Si la soci√©t√© nous rajette. C'est parce qu'elle veut oublier que c'est elle qui nous a cr√©√©. L√†. Il dit le blouson noir a une fonction devant les hommes. Et m√™me devant Dieu. Tu sais pendant un moment il y avait les pr√™tres ouvriers. Ils voulaient √™tre pr√™tres blouson noir. Ils en parlent souvent il para√Æt qu'il a exist√© Dieu. Il para√Æt que Dieu t'a √ßa un acte. Quand il a r√©agi. Il est arriv√© sur la terre. Il y avait rien. Il y avait rien. Il y avait pas un troquet pour une mobilette. Rien. C'√©tait vraiment la zone. Il para√Æt que Dieu c'est lui qui a dit. L'homme est √©gaux. Il para√Æt que Dieu il a dit. Il y aura des hommes blancs. Il y aura des hommes noirs. Il y aura des hommes petits. Il y aura des hommes grands. Il y aura des hommes beaux. Il y aura des hommes moches. Et tous seront √©gaux. Mais √ßa sera pas facile. Et puis il a m√™me ajout√©. Il y en aura qui seront noirs, petits et moches. Et pour eux √ßa sera tr√®s dur. Enfin moi je vous raconte √ßa. C'est l'√©vangile selon Bobby. Mais para√Ætrait que √ßa serait pas lui qui aurait dit. Il y en a qui bosseront 8 heures par jour. Et d'autres qui auront qu'√† se baisser pour les ramasser. Puisque les mecs qui bossent sont assez cons pour se laisser faire. C'est pas lui √ßa. C'est un autre barbu qui est venu plus tard pour bouffer dans sa gamelle. Un mec qui a le masque. Un nom comme √ßa. Qui a le masque. Bobby il peut pas le saquer. Si c'est un de vos potes vous lui dites qui se cache. Parce que nous on d√©conne tout le temps de dire Dieu. Mais Bobby dit. Voyez les mecs. On d√©conne de dire Dieu. Mais lui tu vois il s'en fout. Parce que Dieu il sait au fond de lui-m√™me. Que J√©sus-Christ et la caravane pensent. Merci.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "import whisper\n",
        "model = whisper.load_model(\"large\")"
      ],
      "metadata": {
        "id": "RI0_irFfSpuX",
        "outputId": "79b586be-1c6e-429d-add0-82277ba8eebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-lnij3vnk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-lnij3vnk\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=2a058d84e119c5199df99b057b7c338008b1b6456416e8e1b88ed6d596a62d90\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v4toulge/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.88G/2.88G [00:30<00:00, 99.7MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download a transcription file\n",
        "from google.colab import files\n",
        "!zip -r download.zip download\n",
        "files.download(\"download.zip\")"
      ],
      "metadata": {
        "id": "fKEdUXyRrDIE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}